{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987cfca6-ec9d-4ba5-811a-9fa9b12ace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b5b3e-6bd6-495f-8420-48d152439508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer(nn.Module):\n",
    "\n",
    "    def __init__(self, bits, scale_init=None):\n",
    "        super().__init__()\n",
    "        self.bits = bits\n",
    "        self.scale = nn.Parameter(torch.tensor(scale_init if scale_init else 1.0))\n",
    "        self.soft_bit = nn.Parameter(torch.zeros(1))  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            q_step = self.scale.detach()\n",
    "            x_q = torch.clamp(torch.round(x / q_step), -2**(self.bits-1), 2**(self.bits-1)-1)\n",
    "            return x_q * q_step\n",
    "        else:\n",
    "        \n",
    "            q_step = self.scale\n",
    "            x_q = torch.clamp(torch.round(x / q_step), -2**(self.bits-1), 2**(self.bits-1)-1)\n",
    "            return x_q * q_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b54ae-5f40-473e-abe9-0faeb38518ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizableResNetBlock(nn.Module):\n",
    "    \"\"\"Wrapper for ResNet blocks with quantization\"\"\"\n",
    "    def __init__(self, block, w_bits=4, a_bits=4):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.w_quant = Quantizer(w_bits)\n",
    "        self.a_quant = Quantizer(a_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize weights\n",
    "        for name, param in self.block.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param.data = self.w_quant(param.data)\n",
    "        \n",
    "        # Quantize activations\n",
    "        if self.training:\n",
    "            x = self.a_quant(x)\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a897641-1a4f-4d02-8440-88159f8ffecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_temp_decay(current_iter, max_iter, start_temp=20, end_temp=2, warmup=0.2):\n",
    "    \"\"\"Temperature decay for rounding loss\"\"\"\n",
    "    warmup_iter = warmup * max_iter\n",
    "    if current_iter < warmup_iter:\n",
    "        return start_temp\n",
    "    progress = (current_iter - warmup_iter) / (max_iter - warmup_iter)\n",
    "    return end_temp + (start_temp - end_temp) * max(0.0, 1 - progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bfa47-0543-4540-9757-5feadebc2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_resnet18(teacher, student, genie_data, num_iterations=20000, batch_size=32):\n",
    "\n",
    "    w_params = []\n",
    "    a_params = []\n",
    "    \n",
    "    for module in student.modules():\n",
    "        if isinstance(module, Quantizer):\n",
    "            if hasattr(module, 'weight'):\n",
    "                w_params.append(module.scale)\n",
    "            else:\n",
    "                a_params.append(module.scale)\n",
    "    \n",
    "    optimizer = Adam([\n",
    "        {'params': w_params, 'lr': 1e-4},\n",
    "        {'params': a_params, 'lr': 4e-5}\n",
    "    ])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_iterations)\n",
    "    \n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        idx = torch.randperm(len(genie_data))[:batch_size]\n",
    "        x = genie_data[idx]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_out = teacher(x)\n",
    "\n",
    "        if torch.rand(1) < 0.5:\n",
    "            student_out = student(x)  \n",
    "        else:\n",
    "            student_out = student(x)  \n",
    "        \n",
    "        recon_loss = (student_out - teacher_out).pow(2).mean()\n",
    "        \n",
    "        temp = linear_temp_decay(iteration, num_iterations)\n",
    "        round_loss = torch.tensor(0.0, device=x.device) \n",
    "        for module in student.modules():\n",
    "            if isinstance(module, Quantizer) and hasattr(module, 'weight'):\n",
    "                round_loss = round_loss + (1 - (2 * torch.sigmoid(module.soft_bit) - 1).abs().pow(temp)).sum()\n",
    "        \n",
    "        total_loss = recon_loss + 1.0 * round_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if iteration % 1000 == 0:\n",
    "            print(f\"Iter {iteration}/{num_iterations} - Loss: {total_loss.item():.4f} \"\n",
    "                  f\"(Recon: {recon_loss.item():.4f}, Round: {round_loss.item():.4f})\")\n",
    "    \n",
    "    student.eval()\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb76840-0e62-4952-a42d-3e01c5eee226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data generated from Genie D\n",
    "synthetic_data = torch.load('dataset_checkpoint_final.pt', map_location=torch.device('cpu'))\n",
    "synthetic_data = synthetic_data['dataset']\n",
    "synthetic_data = torch.tensor(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea83e7e-4cc1-4513-b4e9-0742eeff27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    teacher_model = resnet18(pretrained=True).eval()\n",
    "    student_model = resnet18(pretrained=True)\n",
    "    \n",
    "    for name, module in student_model.named_children():\n",
    "        if isinstance(module, torch.nn.Sequential):  # For ResNet layers\n",
    "            for i, block in enumerate(module):\n",
    "                module[i] = QuantizableResNetBlock(block, w_bits=4, a_bits=4)\n",
    "    \n",
    "    # Run reconstruction\n",
    "    quantized_model = reconstruct_resnet18(teacher_model, student_model, synthetic_data)\n",
    "    \n",
    "    torch.save(quantized_model.state_dict(), \"quantized_resnet18.pth\")\n",
    "    torch.save(model, \"quantized_resnet18_full__.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5d5ea-8a1a-4384-9eaa-26e79620c166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
